This is how to build a fake OpenAI server with Llama CPP, so you can automate finance tasks with AI on your desktop computer. OpenAI is obviously hugely popular, but what if you wanted to run your own AI locally, without being tied to using their API? Well, Llama CPP allows you to do exactly that. With it, you can run some of the most powerful large language models, like Mixtro and Llama, but the kicker? You can run them on just about any old computer, without the need for a GPU. But it gets better. Deep inside the library, there's a function that spins up a server that mimics OpenAI. This means you can swap out OpenAI for Llama CPP and run LLMs in your apps for free. I'm going to show you how to do it with a little finance flair, but will it stack up to the speed and performance of GPT-4? And what about more advanced techniques like function calling and multimodal models? I've been working on this for weeks, building prototypes while traveling, integrating with the API, running different LLMs, and coding over 831 lines of experiments. I'm going to break it down in just five steps, and it begins with starting the server. G'day, lads. So today, we're going to be building our fake OpenAI server, and that's going to allow us to do a ton of amazing large language model things. Now, don't be afraid of the code that's on the screen. These are actually the steps that we can use to get this up and running, and it's eventually going to be on the GitHub link below. So the first thing that we're going to need to do is set up Llama CPP, because this is the backbone of everything that we're going to be doing. So I've got this first command here where we're going to clone it down. It is from this site. So if you do get stuck, just know there's a whole bunch of information. It's really well supported, and there are a lot of people using Llama CPP right now. So you're in good hands if you're thinking about using this. Okay, so the first thing that we're going to do is inside of the folder that we want to work in, we are going to be cloning Llama CPP. So let's make this a bit bigger. So we're going to git clone this. Let's not make it caps. And boom. So git clone and then the link to our Llama CPP repo. So if I go and run that, that's going to clone it down. And then next, what we're going to do is we're actually going to build this package. So to do that, we need to go into that folder. So right now, if I type in LS, you can see that I have my Llama CPP folder there. And if I open it up, you can see that I've got it there now as well. What we actually need to do is make that library. So this is because it's built inside of C++, hence the CPP in Llama CPP. And if you actually scroll on down, there's the installation instructions which come here. So over here, you can see on a Mac, we just need to run make within the folder. On Windows, it's a little bit more extensive. Highly recommend you use Windows subsystem for Linux if you're going to be running it on Windows. For now, I'm running it on my Mac. I know it's a little bit of change from the usual. So we're going to hit clear. And then what we're going to do is we're going to go into that folder. So CD Llama CPP. And then we're going to run the make command. So if I just type in make here, this is actually going to go on ahead and build it. So we'll be right back as soon as that's done. Five minutes later. All righty. So that is now done. So looks like we're all good. We can clear that now. And then what we're going to do is we're going to jump out of it. And so if we actually go and take a look in our installation command. So we've now gone and completed step two. We can now go and install the Python libraries that we're going to need. Because we're actually going to interact with our fake OpenAI server using Python. So we can actually copy this command here. So pip install OpenAI. So we're going to be using the OpenAI library. But we're really going to be faking it. So this is going to be free. You don't actually need to pay for tokens, so on and so forth. But you get to use amazing LLMs ridiculously quickly. It's insane how fast this is. And then we're going to need the Llama CPP Python library. We're going to need Pydantic for a little special project later on. We're also going to need Instructor and Streamlet. And Streamlet's going to come in the next part. So let's copy this over. And then I'm just going to paste that at my line. Boom. So this is going to install all the Python libraries that we need. And I've got them installed already. So they're installing pretty quick. If they do run a little bit slow for you, that's perfectly okay. And then what we're going to do is let's take a look at our next step. We are going to start our server. So this is actually going to spin up our fake server literally that quick. And we're able to get this up and running. So if we wanted to just get it running, we can run this command here. So I'm going to copy this. And I'm going to clear my command lines. I've got a bit of room. So it's python-m. I'll actually write it out for you. So let's go and write this out. So the full command is python-m. And then we want to run Llama CPP. So it's Llama underscore CPP dot server. And then we need to specify which model we actually want to load in. So to do that, we can pass through some commands or some flags. I'm going to pass through dash dash model. And then we need to specify the path to the model. Now, I've gone and downloaded some models already. And I've got them stored in here. I'm going to include all the links to the models that I've used inside of this markdown file. So over here, I've got Mistral. We're going to use Mistral to begin with. And we're specifically using the gguf model. So these allow us to load these into a regular old computer. In this case, I'm using my Mac. Could also use a machine that's empowered with CUDA. But over here, we've got a whole bunch of different quantized versions that we can go on ahead and use. Any of these will work. I am using the, which one am I using? I am using the, what is that? The 4-bit quantized model over here. So you can see that I've got that file over there. So we're going to choose the path to that. So over here, I've got all of those models inside of a folder called models. Let me show you what this looks like just on my desktop. So over here, if we go into React, so you can see I've got that models folder right there. You can see, cool. And if I step into that, got all of those quantized models. So we're going to load these, right? So by using a quantized model, it basically means that we're going to be able to load it into our regular machine. And it's going to run a hell of a lot faster. So we've gone and written python-mlama-cpp.server. We now pass through the path to the model. We're going to specify models and then forward slash the model that we want to load. So I want to load Mistral. If you wanted to use other models, you can actually go and just dump those in or pass the path to those models as well. Plus a little bit later on, we're actually going to be using a config file, which allows us to load in multiple models at once. Okay, so that is our baseline model. Now, if I wanted to run this using a GPU, I just need to pass through one flag. Let me run it without a GPU first, and then we're going to load it with a GPU. In terms of what it actually looks like when you use GPU versus non-GPU, speed is going to be a huge factor. But you can see down there, that is loading right down at the moment. So we've got this little progress bar. And take a look, we can see that our server is now up and running. So we've got our server running at http://localhost8000 down there. But we don't want to run it without a GPU. So right now, you can see that we haven't offloaded any layers to our GPU. That's no bueno. It's going to be slower than we want it to be. So let's go and load it up using a GPU. So I'm just going to rerun the exact same command. The only thing that we need to do, so you can see that we've passed through the model flag right up here. The only other thing that we need to do is pass through __n__gpu and set that to negative one. So that's going to offload as many layers as it can to the GPU, as many of the transformer layers to the GPU. That means it's going to run a hell of a lot faster. And take a look, that's it on GPU now. So if we scroll on up, this is now using my Apple M1 Max and it's running on Metal. And over here, you can see that we have offloaded layers. So it says offloaded 33 out of 33 layers to our GPU. This means it's going to run way faster. Okay, cool. So that is that. But so far, I haven't actually showed you how to use this. We need to get some validity or some actual productivity out of this. So let's do a little bit of coding. So we're going to create a new file and we are going to call it app.py for now. Keep it simple. Okay, so what we need to do is we are going to build up a really simple script to actually go and run a prompt using what we just started up. So right now, we've got our server running. You can see that it's running at this API down here. So localhost 8000. How do we actually go and use that? Well, this is exactly what we're going to do. So remember, we went and installed a couple of Python libraries, one of which was OpenAI. Now, we're not actually using OpenAI in any way, shape, or form. We're not actually going, you don't need an API key. You're not actually using their service. So this means that if you want to use your own data and you don't want to send that across the net, this is perfect. So let's do this. So we're going to write from OpenAI, import OpenAI, and then we're going to create a client. So our client allows us to interact with the API that we just spun up. So we're going to write client, create a client, and then we're going to specify our client as OpenAI, and we're going to specify our API key. And you're like, Nick, you said no API key. You can literally just type in gibberish in there. It doesn't matter because we're using our local instance. The way that we use our local instance is by typing in base URL and pointing it to our local host environment that we just started up. So this means that when we go and communicate with our server, we're actually not going out to OpenAI at all. We're literally just using the Python library. That is it. So it's completely open source. We're literally going to our local server that we started up. I know I keep harping on about it, but it's so important when it comes to data security. All right. So to go to our base URL, we're going to type in HTTP colon forward slash forward slash local host. And remember when we started it up down here, it was running at local host 8000. So we're going to point it to local host. Let's come back. Local host. Come back here. 8000. That keeps scrolling down. And then we need to pass through really important forward slash v1. So that means that when we go and use this client now, we're actually pointing to our little server that we just spun up. Kind of cool, right? So it's not that hard to get up and running. All right. So that's our client. But now we actually need to go and run something against it. This is.
as of right now, we've just got our client set up. Let's just toggle on WordRap so we can see exactly what we're running. So we've gone and specified our API key, gone and specified our base URL. We're now going to perform some chat completions. So chat completion here. So we're going to create a new variable called response, and we're going to set that equal to client.chat.completions.create. So basically this allows us to send a bunch of messages to our API, our fake OpenAI server that we just spun up using Llama CPP. To do that, we need to specify our model. And right now I'm going to get you into a good habit, right? So we're going to specify which model we want to use. Which model we want to use. And for now we're just going to say Mistral, but right now that's not actually pointing to anything because we haven't actually gone and set up aliases, but we will eventually. So right now we're going to say that we're going to be using Mistral. I like as a good habit to just go and specify which path. So like keep in mind that this doesn't really matter at least for now. So that is the model that we are using here. Now we also need to pass through some messages, pass through our prompt. So how do we do that? Well, we can specify another keyword argument and we're going to specify messages and we're going to set that equal to an array. So you can see that I've gone and specified an array down here. This is because we can send multiple messages. And then to that, we're going to pass through a dictionary and then our dictionary needs two keys. So our first key is going to be the role. So who is sending this message or what type of prompt it is. And our role in this case is our user. So we're just a regular old user sending a prompt to our fake OpenAI server. And then we actually want to specify our prompt and our prompt is set in a key called content. So our prompts could be just about anything, right? So let's say for example, we wanted to learn a little bit about, I don't know, a finance metric. So let's think of a finance metric, return on investment. So we might say, what is ROI in reference to finance, right? So that's going to be our prompt. So hopefully we should get, this is all going to have a finance theming in case you missed that. So that is our prompt over there. So I've gone and specified our role, which is our user and our content, which is going to be what is ROI in reference to finance. I'm going to save that. So it's going to do a little bit of prettying of our code. Cool, so we've now gone and specified our response. How do we actually go and print this out? Well, we're just going to print out the raw response and then I'm going to show you how to pass it. So you can actually see all the different variables in it. So print it out. Coolio, all right. Now to run this, we can just run Python app.py. So let's go and run this. So I'm just going to create, so we need to keep our server up and running. So don't close this command prompt or this terminal when you've got that up and running. We're just going to create another one here. So I'm going to now run Python. My head blocking that. Yeah, Python app.py. So all things holding equal, take a look. So we've now got our response. This is actually coming from our fake open AI server. How amazing is that? You're probably looking at this like, Nick, what on earth are you printing out? There's a lot of information here. Well, this is the raw response from our fake open AI server. But if you actually take a look a little bit closely, this is actually going and giving us a response that is actually pretty relevant. So over here, we've got this content key, which says ROI stands for return on investment. In context of finance, it is a measure of the profitability of an investment. It represents the ratio of net profit generated. But right now it looks a little bit messy, right? Like, so we've got a response, but like, you're probably like squinting, going like, what on earth is that saying? Like, it's a little bit tricky to see. So we can actually make that a lot clearer. So we can traverse this response. So what we need to do is inside of here, we've got a variable called choices here. So we need to go into our choices attribute. We then need to grab the first value because it's inside of an array. And then we need to go and grab our content over here. And our content is stored inside of another variable called message. Sounds complicated, not that bad. So we're going to go response.choices. We're going to grab the first choice, which is index zero. And then we, let's go and take a look again. We need to go, so we've got choices. We need to then grab message, and then we need to grab content. So we can type in .message.content. I'm showing you how to traverse this because if you wanted to pull out any of the other stuff, you'll be able to do that. All right, so if we go and print this out now, we should hopefully get just the raw response, not all the other crap as well. But you do get some other useful stuff, right? So you get the role of the assistant coming back. You get whether or not any tool calls have happened. I don't believe that's working yet, just full disclosure. We get when it was created, the model that was used, system fingerprint. We get the number of tokens that were used down here. We also get the number of prompt tokens. We get the total token. So these are the completion tokens. So the response effectively, plus a number of prompt tokens. So a lot of information there. So if we wanted to go and get this other stuff, then we wouldn't just traverse the response. So, but for now, let's go and rerun that. So we can rerun it using pythonapp.py. Hopefully we just get the response and take a look. Just like that, you've gone and built your own fake open AI server. And you've now got a response. Give yourself a clap, guys. That is absolutely amazing. If you got it this far, brilliant. Let's read it out. The ROI stands for return on investment. It's a financial metric used to evaluate the profitability of an investment by calculating the net return generated from it. Expressed as a percentage of the initial cost or investment amount, in simple terms, it measures how much profit you make on your investment compared to how much you spent on it. A high ROI indicates that an investment is generating significant returns relative to its cost, while a low ROI suggests that the investment may not be profitable. Not too bad. This is great, but we can make this better. And one of my favorite ways to make this better is to add streaming. So we're going to keep building up on this. So we're keeping it simple, but we're going to go pretty hardcore. But I'm going to take it step-by-step and I'm going to give you everything you need to know. So we're going to say stream equals to true. And rather than just printing out this now, right, we actually are going to use a generator to actually print out each token as it comes. And this shows you just how fast this runs now, right? So let's go and print this out or use a stream. So to do that, we're going to loop through each chunk we get back from our response. So we go for chunk in response. Then what we want to do is first up, we want to check whether or not, or for now, let's just print out all of the chunks. So I'm going to copy this entire response and we're going to print this out. So this is the original one. So print, looking good. And I'm going to comment this out because we don't need that. So rather than it being response.choices.message, it's now response.choices0.delta.content. So that should effectively print out our streamed tokens. Now we're going to implement one additional thing just to make it look a lot cleaner. And this is effectively going to give us a full blown streamed on our command line, which I think looks really nice. So we're going to say flush equals true. And we are also going to specify end equals, it keeps moving, end equals a blank value over there. So effectively this should stream out our response now when we go and run this command. So this is going to be streaming the response out. Okay, beautiful. So let's go and test this out. So all things holding equal, this should now just go and stream out our response. So it would be like, effectively like machine gun out to our command prompt. Okay, so let's go and run this. So python app.py again, and we've got an error as stream object has no attribute choices. Let's print out the raw stream. Maybe I've typed in something wrong. Let's print out the chunk. You thought there would be no errors. I like showing you the errors. This is what happens. All right, so that's out that you can see it's streaming. It's gone super quick. I've clearly just typed in one value wrong. All right, so we need to go. So I've tried to go choices. So it does have choices. So choices zero, and then we need to go delta. That should be, oh, I know what I've done wrong. This should be chunk.choices, not response. It was all fine. All right, so chunk.choices, and then we're going to grab the first choice, grab the delta, grab the content. All right, let's rerun this. It's definitely streaming. So it's streaming out. We just need to clean it up. I'm going to type in clear. Let's run this now. Take a look. How awesome is that? So it's now streaming, and look how quick it is, right? It's ridiculously fast. And there's other models. You can obviously use mixture. It's going to take a little bit more time to generate the output, but it is still performing extremely well. I don't like the fact that it outputs these nones over here and over here. So we can actually get rid of that, right? To do that, we just need to double check that the value is not none. So I'm going to copy this. I'm going to say, if this is not none, then print it out, right? So that's our updated code. So it's effectively going to, if the value is not none, print it out. Cool, so if we go and run this now, let's clear it out. Beautiful, working perfectly. Take a look at that. So just like that, we've now gone and spun up our OpenAI web server, and we've also gone and implemented streaming. Not too bad, but we can make it better. And the way that we're going to make it better is by implementing or converting this into an application. So we're going to clear this, and we are now going to take this one step further. So rather than just having to update this prompt over here each time, we're going to go ahead and just do that. So we're going to go ahead and do that.
time, which is a little bit of a pain, particularly if you've got users, we're going to convert it into an app that you can run on your own desktop. So we set up the fake server and have that LLM streaming pretty fast, but how would we go about hooking this up to an application though? This brings us to part two, building an app for the fake server. Time to throw this bad boy in to an app. So rather than having to go and update a prompt over here, we can actually go and frame this inside of an application, which like, even though we've got a server that's up and running, we want to be able to go and use this inside of different applications. So you can see that we're getting a better API is taking all of these calls and we're able to go and use tokens, but we really want an application, right? So we're going to jump back into our app.py file, and we're going to do exactly that now. So let's do it. So the first thing that we need to do is we're going to import Streamlit. So Streamlit is going to be the app framework that we're going to use. So we're going to say, import Streamlit as ST. Now you've probably seen me build a bunch of Streamlit applications before, this one's going to be pretty similar. So over here, what we're going to do is we're going to create a title. So I'm going to say ST.title, and you can name it whatever you want. So I'm going to say, let's throw in an emoji. So rocket, llama, see, actually let's say fake, open AI server app, dot, dot, dot, llama CBB. You can name your app, whatever you want. So that's going to be my title here. So the title of the app, then what we're going to do is we're going to create somewhere that we can hold a prompt. So we're going to say prompt is equal to ST.chat, input, and inside of there, we're going to say, pass your prompt here. So that is where we're going to be able to store our prompt. Then what we want to do is if somebody goes and types in a prompt and hits enter, we want to go and effectively go and do all of this stuff that we've been doing so far. So we're going to say prompt, or if prompt, then the first thing that we'll do, so if the user hits, types a prompt, and hits enter, then what we're going to do is we're going to effectively run our app. So we're going to, first up, we want to render our user's prompt message to the screen. Relatively easy to do. We can type in ST.chat message, and there's a ton of information on Streamlit, right? So if you type in Streamlit chat components or chat elements, there's a ton of information about all the different chat elements that you can use. So I tend to use ST.chat input and ST.chat message the most, but there are a bunch of others as well. So the chat message, the first value that we need to pass through is who was passing through the message, because that's what dictates how, or the chat formatting, as well as the little icon that you see, or the avatar that's on the side. Chat message here, we're going to say user, and we're going to render markdown, and to that, we are going to pass your prompt. So this is just going to render the user's message to the screen. Cool. Then what we want to do is we want to take that prompt and we want to send it to our fake OpenAI server. So we're going to, over here, inside of our content tags, we are going to pass through or change that to our prompt. So this previously held our ROI prompt. Now we're just going to pass through our raw users prompt. So we're taking this, whatever they type into the chat input bar, and we're going to be sending it to, then we're going to embed, we're going to be sending it to our client.chat.completions.create endpoint. So that is looking bueno, and what do we need to do now? So once we've got that response back, we want to go and render that to the screen. Now, my personal favorite way of doing this is again, streaming, because it just shows you just how fast it is. And it sort of reaffirms to the user that, hey, this is using a large language model. Cause like sometimes you'll see stuff that isn't streamed and you're like, this could just be stuff that's printed to the screen. I like showing streaming for that very reason. Okay. To do streaming inside of Streamlit, we can keep the majority of what we've got here. We just got to do some extra magic. So we're going to say, with st.chat, why have we gone gray? Oh, it's cause we haven't used that yet. With st.chat underscore message, we're going to say AI, then, why is that gray? That looks fine. Okay, we'll wait and see. Response. Oh no, we'll see. So with st.chat message, and then to that, we're going to specify AI. We are then going to create an empty value, or we'll, let's say a message variable and set that equal to st.empty. So this is eventually what's going to render our output. We also want a variable called completed message, and we're going to set that to a blank string. So then what we're going to do is we're effectively just going to tab in the chunked response bit that we had down there. And we are going to say for chunk in response, if the chunk.choices, we're not going to print it out anymore. All we're going to do is we're going to take that chunk and we're going to append it to our completed message first up. So this will effectively build up a variable that holds our entire completed message. We're going to say plus equals this over here. And then the last thing that we are going to do, it's killing me that this has gone gray. Why is that gray? Or like dark? I don't know. We'll dig into that in a sec. So we're going to take our chunk and we're going to append it to our completed message. And then we're going to take this over here, our message value, and we're going to update that using our completed message. So if I copy that, boom. Okay, that's all good. It's still grayed out. Can you see that on the screen? Yeah, it is coming through. Better? Nope. We'll soon find out if there's an error. Let's stick with that for now. So now in order to start our app, whatever, it's not highlighted. We'll get there. So in order to start up our app, we can run Streamlit, run app. Let me bring that up a little. Stop by. Beautiful. Okay, so this is our application now running. So we can now go and say, write me a Python function to get stock prices using Y-Finance. And, oh, we've got an error. Hold on. That might be it. Yep, there you go. There's our error. Beautiful. Now we're back highlighted. I knew we'd find it. Let's try that again. So write me a Python function. Let me zoom in on this. To get stock prices using Y. Let me scroll over using Y-Finance. So this should, take a look at that. It's running. This is now sending it to our fake OpenAI web server. It's now going and writing a Python function and it's printing it out. So we've now taken our baseline fake OpenAI server or our Llama CPP compatible web server and we've now gone and wrapped it inside of an application. So we can actually go and pass through whatever prompt that we want. And you can see here that the, remember how I mentioned, so the chat message dictates the avatar and the formatting. So because we specified .user, we get that little avatar. And because we specified AI, we get that little avatar. But take a look, it's actually writing a function to be able to go and extract our ticker prices using Y-Finance, which we're going to be using in a second. But for now, we've now gone and wrapped up our web server and use it for an application. App is done and dusted. And we're even able to use open source LLMs to write Python into the Streamlit app. But so far, this isn't really anything all that revolutionary. What if we could use the LLM server to call out to functions? Maybe even get it to do some stock analysis for us. This brings us to part three, function calling. So we've now gone and wrapped up our application inside of our app. It's time to get onto my personal favorite bit, function calling. So we've done a lot of chat, but what happens if we want to go and integrate this into other applications, if we want to go and do some cool stuff? Well, we can do that as well. I'm going to show you how. So to do this, we first up need to import another library, and we're actually going to build up a function calling framework to be able to go and extract stock prices over a set period. So let's go and do this. So we're going to build it up over time. So we're going to bring in the instructor. Library. And so this allows us to go and build up a response model and effectively extract the values that we need from a prompt. So we're going to go import instructor. And then what we need to do is we need to patch our client to be able to go and use this response model, which you'll see in a second. So for now, we're going to say client is equal to instructor dot patch. And then we're going to say client is equal to client. So basically we're taking our key or we're setting our keyword argument to our client over here. So we're now going to say create a patched, patched client. Beautiful. Okay, so that is our client. Now it's patched. Now what we need to do is we need to go and define how we want to go and extract these values. So I've actually got a function over here, which I'll include in the GitHub repository, which takes in two values. So it takes in a ticker as a string. So this might be the stock ticker AAPL for Apple, and it takes in an integer. So the number of days. So this represents how many days of data that we want. So, help.
Really, we need to be able to go and extract a ticker as a string and the number of periods that we want, so days as an integer. So we can actually define a response model, which is actually gonna integrate with our web server to be able to do that, which means that we'll eventually be able to use our LLM to call this function. So we are gonna build this up. I know, this isn't like, I've been wanting to teach this for so long. So let's first up, what we need to do is we need to go and create a class. So we're gonna create a class called our response model. And then this is going to structure what we want extracted. I'd love to do like a detailed video just on like building like a hardcore function calling project. Let me know if you want that in the comments. So we also need to bring in the Pydantic library and specifically one class from there. So bring in the base model class. So we're gonna say from Pydantic import base model. And then we're gonna pass base model over here. So we've just gone and written from Pydantic import base model. So this base model is going to form a structured effectively response that we're going to be able to get out from our large language model. So this is one of my favorite things, right? So we can specify what we want to extract from our generated output or from our prompt. So what we actually need is we need to get back. Remember our ticker as a string, and we need to get back the number of days as an integer. Super easy. We can type in ticker string, and we wanna go and specify our days as an integer. So basically we just define what values we want our LLM to extract and the data types that we want them extracted as. And we're able to get this back. I know, absolutely brilliant. We can now go and define that. We need to make one tweak to our chat.completions.create. So we're no longer gonna stream at least for now. What we need to specify is the response model. And we're gonna set that equal to our response model over here. Beautiful. And then what we also need to do is we need to re-spin up our web server to be able to use this. So specifically we need to change the chat format that our fake web server is currently using, or our API is currently using, to be able to handle this response model. So the way that that occurs is using the functionary chat format. So if I type in functionary, open AI web server llama DPP, you can actually see that there's a whole bunch of documentation around actually doing this. So I can type in functionary, and you can see that we can go and specify the chat format over here. So we're gonna do exactly that. I'm gonna show you how to do it. So let's go do it. So we need to go back to our server, and we need to stop our server. So we can do that by hitting command or control C on our keyboard. And we're gonna spin up our server again, but we're gonna add in one additional flag this time. So we're gonna say dash dash chat, functionary. So this is telling our web server that we wanna go and use the chat, the functionary chat format. By default, I believe it's chat ML. We're gonna choose functionary this time. And eventually once we get to our next stage in our journey or the next part of this, we're actually gonna be able to use multiple models, which makes this a whole lot more valuable. So we're gonna specify that we wanna use functionary, hit enter. That looks like it's all up and running. And now what we're gonna do is rather than streaming this out, because remember we've disabled streaming over here, we're just gonna respond. We're gonna comment this out for now. We're just gonna respond with our output, which is our response. So we're gonna say st.chat message, ai.markdown is equal to response. And all things holding equal, this response should return back two values or should return back ticker and days. So we can then go and pass that to our function. So let's go and test this out now. So we've commented all that out and we're gonna progressively build up on this. So eventually you're gonna be absolute wizards when it comes to Llama CPP and your fake OpenAI web server. So we're gonna run our Streamlit app again. You don't need to restart it, I just did. And then what we wanna do is we're gonna change our prompt this time. So we're gonna say, summarize the stock price movements for AAPL for the last seven days. All right, so that's going to be our prompt. So summarize the stock price movements for Apple for the last seven days. So if this works, we'll get back two values. We'll get back AAPL and seven days or seven inside of two of our variables from our response model. So let's actually go and run this. Take a look, how awesome is that? So we're now gone and extracted our ticker and we've extracted our days. All that's left to do is actually go and send this to our function. So let's go on ahead and do this. Cause then what we'll be able to do is do things like actually summarizing those stock price movements. We'll be able to use different models to go and do different analytics as well. So that is that, let's go and run our model now. So I'm just gonna close this for now. So, so far, let's quickly take a look at what our code looks like. So we've gone and got all of our imports, just gone and defined our response model. Now over here, what we now need to do is we need to take the output that we got from our response model and we actually need to run our function. So I'm gonna leave this output for now just to double check that we're always extracting it. But what we wanna do is we wanna try to run our function. So our function is inside of a file called stockdata.py. So you can see it there. So we wanna go and extract the get stock prices function from stock data or we wanna import it from that module. So we're going to do that inside of our app.py file. So let me zoom out just so you can see it. So up the top, we're gonna bring in the stock prices function. And you could build it like a ton of functions. You could loop all of these or make them sequential, do a ton of stuff like that. Literally so limitless what you can do with this. Again, maybe we'll do a bigger video on this. So we're gonna say from stock prices, stock data import, is this stock data? Yes, it's called stock data import. What are we importing? Get stock prices. So the function that we wanna run is get stock prices. So right down here, after we've just gone and printed this out back to our application, we're gonna say, we're gonna put this in a try except loop. So we're gonna try, and now we'll pass. And then we're gonna say except exception as E. If something goes wrong, then we're just gonna return something went wrong to the screen. So we're gonna say something, that's a bit too small. We can barely see what we're doing. Something went wrong. Perfect, let's add a sad emoji, sad. Cool, so if something goes wrong, we're gonna return something went wrong with our sad emoji. Now, if something goes right, we wanna return back the stock prices from our stock app. So let's go and do this. But to run our function, we can go and say, get stock prices. And then to that, we are gonna, remember, we need to get our ticker. So we need to get this ticker, and we need to get the number of days to pass them to our function. Super easy to do. We can go response.ticker and response.days. It's literally it, kind of cool, right? And this should return our prices. So we're gonna store that in a variable called prices. Then what we can do is we can render that back to the user. So we're gonna copy this. And instead of returning something went wrong, we're going to return back our prices over here. You see me looking up here, it's just to make sure my head's not blocking the code. So we're gonna pass our prices over here. So if that works, we should get back our raw response from our prices. So let's go on ahead and do this. So let's go and read. We're just gonna copy our prompt, refresh, pass it through in here. Take a look at that. We've now got our prices now extracted. So you can see in here, we've actually got a JSON object that contains all of our prices. So we've got the date, and then inside of that, we've got our open value, our high value, our low value, our close value, our adjusted close and our volume for each one of those dates. So we've now successfully gone and used our LLM to call a function. Now, what would be really nice is to actually go and use that to summarize the stock price movements. We'll be doing that in a sec. So we've got the server to call out to our function and bring back different stock prices, but that's only halfway there. We didn't actually summarize the stock price movements. To do this, we've got to send the data back to another LLM. This brings us to part four, using multiple models simultaneously. Now, what we need to do is we actually wanna go on ahead and use a different model to perform the summarization. So far, we're calling the function, but we're not actually summarizing. So kind of haven't hit the brief yet, but we will. So what we're gonna do is rather than using the functionary model, because remember we went and spun up our server to be able to go and use the functionary tool, which are we actually the functionary chat format. We now wanna go and use a different model and we wanna use chat and ML. So we wanna go and use that same format that we started off with, but now we actually want to run them both at the same time, because we wanna go and extract those tickers and we wanna go and do the summarization. How do we do that? Because our server's only handling one model at the moment. Well, this is where multiple models come in. So we're gonna stop our server.
and I'm going to show you how to do this. So over inside of the GitHub reaper, I'm going to give you this config so you can go on ahead and use it, but you are able to go and spin up your fake server with the multiple models at the same time. All you have to do is define a JSON object. In this particular case, I've gone and prepared one for you, but you can go and specify a whole bunch of different models, a whole bunch of different chat formats. You can also specify whether or not you want to use your GPU or not, number of threads, batch size, the context window as well. So context is here if you need a bigger context window, also really important. If you want it to run on a different port or on a different host, you can do that as well. So this means that it's still going to run on port 8000. Now, I've set this up so that it's going to run three different models at the same time. It's going to run Mistral over here. And the beautiful thing about this is that we're able to specify a model alias. So over here, you can see I've specified the model alias as Mistral. Then I've also got another model over here, which is the big chungus, Mixtral, which is the big boy. So this is the eight by 7 billion parameter model. And the alias that I've got there is Mixtral. And again, this is using the chat format, chat ML. And then I've gone and specified Mistral again, but this time I've gone and specified the chat format as Functionary. So we'll be able to go in ahead and use the function calling method there. So the model alias here is going to be Mistral-Function-Calling. But you could define a bunch more. You could define different aliases, different chat formats, so on and so forth. But I'm going to show you how to spin up the server using this. So it's actually super easy. So instead of using the model flag, like you've seen us do before in the GPU, we can actually go in and simplify this a ton. So we can go and type in Python-M, llama underscore CPP dot server. And this is where it gets super easy. Let me make this a little bit bigger. We're going to specify one flag, dash dash config underscore file. And then what we need to do is point to that file. So we're going to say config dot JSON. So this is because my config dot JSON file is in the same root folder that I'm calling this function from. But if you had it inside of a folder called, I don't know, source files, it'd be source files forward slash config dot JSON. Or if, let's say there was a underscore in the fault folder name, it'd be source underscore file config dot JSON. If you had it inside of a file called, a folder called environment, it'd be environment forward slash config dot JSON. You get the idea. Mine's in the same folder. So all I need to do is specify config dot JSON, hit enter, take a look at that bad boy. It's now up and running. And we can point to all of those different models. Kind of cool, right? So let's jump back into our app because we're going to make this absolutely brilliant. And we've got to do a couple of tweaks here, right? So if we go back down to our model, remember we specified our model name. Now it is important because we need to point to the right alias. So for our actual function calling over here, we want to point to our function calling or our model that has the chat format function area enabled. So we're going to go to our config and down here you can see the model alias that I've specified is Mistral dash function calling. So I'm literally just going to copy that because that's going to point to this Mistral model and it's going to have the chat format function area. So if I go back into my app and then I change this file name over or this variable value over here and I'm going to set it to Mistral dash function dash calling brilliant. So that should now work. That should now point to Mistral function calling. We do need to make another call down here to actually do the summarization. Cause right now we're not doing the summarization or we're not following through on what we wanted our prompt to do. So how do we do that? Well, we just do another LLM call. So this is where we're going to get stuck in. I'm going to zoom out a little bit just so you can see this all. So I'm actually going to copy this chat completion bit over here and I'm going to paste that inside of our try catch block. And then I wonder why Python didn't call it try catch. They made it accept. Let me know if you know why. So this time we are going to specify this variable over here and we're just going to call this full response rather than calling the function call. Let me walk you through. So we're going to run our first LLM, extract the ticker and extract the number of days. We're then going to run the function and then we're going to take the data from the function and we're going to send it to our LLM call in the next prompt. We could do a bunch of prompts formatting. Maybe we do that in the potential full blown function calling course. If you guys want that again, let me know in the comments. So we now need to point to a different model over here. So we need to go back to our config.json, scroll on up. We're going to point to mix draw. So we're going to copy that model alias. And really the model alias, just think of them like nicknames. So that could be, you can name them whatever you want. Most important thing is that when you go and make the call over here, you need to point to the right alias. So we're now going to point to mix draw for our second one because we want it to do some amazing summarization. And then we are going to disable the response model because we don't need it now. We don't need to do function calling or we don't need to extract the ticker and the date. So we've done that in our previous LLM call. And the only other thing that we're going to do is we're going to append the data that we got to our prompt. So I'm going to make, this is going to be a little bit janky, but you get the idea. So we're just going to add in a line break, a backward slash line. And then we're going to append the data that we got from here, which is in the variable prices. You could also do a little bit of prompt formatting and just structure it out. So it's a little bit nicer, but effectively we're taking our original users prompt, which would have been this, summarize the stock price movements for AAPL over the last seven days. So we're taking that, we're then passing through a break. So we're having a space and then we're going to append the prices to that prompt. So now hopefully it's got enough data to actually go and do the summarization because previously it didn't. And then let's enable streaming here. So we can enable streaming again. And then we are going to copy our big streaming chunk over here, big streaming chunk. And we're going to paste that there. And we are going to uncomment this. Also, all the code is going to be available in GitHub as per usual. So let me zoom out. So what we've now gone and done is we've now gone and created another LLM call. So we're now going to be calling mixture with the appended prices over down here. And then we're going to be streaming out the response right down here. The only change that we need to do down here when we actually go and stream out the output is we need to copy our full response and update the response value there. Cause we're no longer looping through the values that we got over here. Cause that's being, that's our function calling response. We now want to go and output it using the summarize response. So let's actually make this a little bit more clear. This is the summary output. So effectively prompt plus prices. This is just the function calling LLM. LLM call. Why not? Cool. All right. So all things holding equal, we should now successfully be able to achieve what our users ask. So summarize the stock price movements for AAPL. And if you're thinking about what's possible with this, right? Like you could take this ticker. You could take the number of days and create an entire profile for a company. Could even plug it into my trading bot that we did in the previous video. So now what we're going to do is we're actually going to refresh this and hope this works. So let's refresh, zoom out a little, paste that back in and see if we got any errors. Something went wrong. Oh no, what has gone wrong? All right. So what I like to do if something actually goes wrong is comment this out and comment this out just so we can see what our error was. Cause otherwise it's just going to say something went wrong. We got no idea. So it's going to be something that's gone wrong down here, I believe. Oh, I remember what it is. I made this error when I was prototyping this. This should be a string. So we're just going to convert our prices and wrap them inside of a string. Let's try that now. All right. Let's paste that in here. Okay. This looks like it's running. It looks like it's being sent to Mixtro. If we actually go and open up our server, take a look, it's loading up Mixtro. So Mixtro takes a little bit of time to load up. That's perfectly normal. It's quite a large model, but all things holding equal, we'll maybe get a summary and see what happens. A few minutes later. Okay. So Mixtro is now loaded into memory. Should be generating a response. Take a look. How awesome is that? It's now generate, we've now gone and successfully implemented function calling and it's generating a text-based summary. Pretty cool, right? We could ask it to find, we could fine tune that prompt and get different responses, but it's actually going and generate and it is the data, right? So on the first day, so what is that? March, the 3rd of March, 4th of March. What are we? Yeah, so 4th of March, the opening price was 176.14, 176.14. The opening price, what was that? The highest price was 176.89. Take a look in our raw data, 176.89. Lowest price was 173.78, 173.78. Closing was 175.1, 175.1. How amazing is that? Volume, did we get volume right? 85, yep. Pretty cool. So in a nutshell, we've now gone and spun up a second model and we've now taken the raw output from our function call and we've now actually gone and done our summary.
Home stretch now, we've got our function calling application up and running, summarizing stock price movements, but what if we wanted to use more than just text data? What if we wanted to use LLMs to analyze images? This brings us to part five, using multimodal models. We're in the end game now, and we're going to deviate a little bit, and we are going to go on ahead and use a multimodal model. So far, what we've done is pretty much follow our path and build up the ability to go and summarize our stock price movements and do function calling. Multimodal is going to give us the ability to go and bring in images and work with them when it comes to using our LLMs. So let's go on ahead and do this. So I'm going to leave our baseline app, and we're going to tweak this, we'll update our baseline app. So we're going to stop our server, and this time we are going to be using a multimodal model. So I'm going to clear this out. And multimodal basically allows us to take in an image and blend that with our text-based prompt. So if you think you've ever uploaded an image to chatGPT and asked it to analyze this image, that's effectively what we've got the ability to do. So if we wanted to go and extract drug labels or maybe look at candlestick patterns, I don't know if candlestick patterns would work, but we could definitely try. We've got the ability to do that. So we've got some really interesting use cases in this space. Okay, so let's go on ahead and let me show you how to load up a multimodal model. One of the most popular multimodal models is called Lava. So over inside of my models folder, I've got two parts. I've got the Lava model, and I've got the Clip model over here. So you need both to be able to go on ahead and run this inside of your fake OpenAI web server. But really, this is the fifth and final thing, which I think is most important when it comes to going and using Llama CPP and the OpenAI compatible web server. It really brings it together. I've also included the links to where you can get these models inside of the readme file. Okay. As well as all the commands to run the server. So if you need a bit of a hand, you got that in there. So let's go on ahead and do this. We're bringing it home. So we're going to type in Python dash M Llama CPP dot server. My head is covering it as per usual, dot server, beautiful. And we are going to say dash dash model, and then we're going to point to our models folder and we can, you could also pass this inside of your config file, but I want to show you sort of just how to do it raw directly on the command line. So it is Llama. What do we need to do? We need to point to the, so this file over here is the Clip model. We need to point to this file over here for the baseline model. We're going to say Q4, and then we need to specify the Clip model path. So dash dash Clip, underscore model, underscore path. And then we point to this file over here, we're going to say models, and then it's going to be Llama. And then we want to point to MMproj, perfect. And then we're going to specify NGPU equal to negative one again. That is the, and we've got an error there, it should be NGPU, not NCPU, rookie error. Beautiful. Let me show you the full command, because I've already gone and run it. The full command is Python dash M Llama underscore CPP dot server. We then specify the model, and we are pointing to the 4-bit quantized model over here. We then specify our Clip model path. I've actually made an error, I just realized now. We then pass through, point to our Clip model path, and we are passing through to the file that has MMproj in it. We also need to specify the chat format. So we need to turn that or specify that as Llama dash one dash five. So let's go back to our server, we're going to need to redo this. So we're going to rerun that command, let's actually just clear it. We're going to rerun this command, and we're going to specify chat, and then that's going to be Llama, I know it's a little bit of pain, one dash five. That is the full command. The Python dash M Llama CPP dot server, we're then passing through or specifying the model flag, we're specifying that value there, or that specific model that we want to use, we're then specifying the Clip model path, and that is that value there. These are just pointing to these files over here, so these two over here. Then we specify the GPU offloading, and then we specify the chat format, which is going to be Llama dash one dash five. If we spin that up now, that is beautiful, that is now running, all things holding equal. OK, so then we're going to go back into our app, and we've done a lot of work on this app now, so I kind of don't want to ruin it, so I'm actually just going to make a copy. I'm going to copy this, paste this down, and I'm going to rename it. I'm going to say this is the Llama app. Cool, all right, so what we're going to do now is I'm just going to get rid of the stuff that you don't need when it comes to using Llama, so we can get rid of Instructor and the base model, don't need that. We also don't need our function. We're going to get rid of that. We can get rid of our Instructor patching, get rid of this response model, so we're going to take it back to basics. And then really all we need is we can actually get rid of all of that. We'll keep our streaming, keep our streaming. Yeah, we'll keep our streaming. Now we've got it down there anyway, so we're going to get rid of this whole chunk over here, which was everything that we needed to do our function calling, so we don't actually need that. We're going back to bare bones. We're going to keep our streaming. We're going to re-enable that and we're going to get rid of this response over here. So really, all we should have back is pretty much the baseline app that we had when we first enabled our streaming. But this time we're going to be using our Llama model. So what we want to do now is we want to, again, that the model aliases don't matter that much anymore because we've gone and used the specific model that we need. We're going to get rid of our response model here and we're going to re-enable streaming. The biggest difference when it comes to using Llama is how you go and pass through the prompt. So over here, you saw that we had content. We got to get rid of that. And to that, we're actually going to replace the content value with a array. And then we're going to have two values inside of our array, we're going to have two different dictionaries. Inside of that, we need two different values. So the first value that we need to specify or the first dictionary is type. And the first type that we're going to specify is image underscore URL, and then we actually need to specify what the image underscore URL is. So let's make this a little bit neater. So over here, the value, I'm just going to put a placeholder here at the moment, but over here, this is eventually going to be the URL for the image that we want to go and analyze. So let's say, for example, we had candlestick patterns, we'd pass through that image URL there. And then the second value that we need to specify is sort of similar. So the type is going to be text. And then this is where we pass through the baseline prompt. So the first value is going to be processing the image so that we can use it. The second value is actually going to be the text that we want to go and pass through. So this is going to be our prompt here, which is just coming from over there. Perfect. So what do we have to do now? So I'm just going to pass through HETV just to make sure we get rid of those errors. So that is looking good. I think that's pretty much good to go. But we need to pass through our URL somewhere. So let's say, for example, we actually wanted to go and use a URL. Where do we get a pass through that URL? So I'm just going to create another input value up here that we can use to hold our URL. So we're going to set this as a variable and say image URL. And then I'm going to pass through a place that our user can specify the image URL. So I'm going to set that to st.let's say text input. And then we're going to say put your image URL. Yeah. Right. So we're now going to have two inputs. So the first input is going to be where a user is able to pass through their image URL. The second chat prompt down the bottom is going to be that whatever prompt they want. So let's go on ahead and do this. So just to quickly summarize, we've gone and updated what we pass through in our messages. So previously, content was just a string. We've now gone and updated it to an array. And that array contains two different dictionaries, one of which holds our image URL and one of which contains our text. And assuming that works, we should be able to go on ahead and stream it out. So let's go on ahead and test out our Streamlit app. Now, keep in mind, because we renamed our app to Lava app, we need to go and start our Streamlit app again. So I'm just going to stop it. I'm going to stop the other one. I'm going to clear it. And then remember, just start up our Streamlit app. What's the command? Probably didn't give you that much help. Streamlit run Lava app. L-L-A-V-A-L. God, this is kind of hilarious. I don't know what it is. There you go. Streamlit run Lava app.py. OK, cool. So take a look. So we've now got an image URL here. So we can actually go and pass through a URL and then specify a prompt and then get it to do something. So let's go and find a candlestick pattern image in keeping with our finance theme. So let's go and find an image. I don't know, let's copy this one. And what we need to do is we need to get the URL. So we're going to open it in a new tab. And as long as it's got JPEG at the end, we need to know, let's go find that that image. Let's go and open this open image in new tab. Perfect. All right. Cool. This should work. Let's get rid of this resize bit. So we've just got the PNG. I'm going to copy that. I haven't tested with this image before. So it's going to be interesting if I paste that in there and say this is going to be our image URL. And then down here, what we need to do is we need to get the URL. So we need to get the URL.
specifies what we want to do, right? So I'm going to say, describe the image provided. Take a look. We've now gone and got a description of the image. So the image features two graphs, each just displaying a different hammer time pattern. Is that a thing? Okay. All right. Clearly a hammer pattern is a thing. I did not know that. One of them is an inverted hammer pattern while the other has a hammer pattern. Both charts are green and red in color, making it easy to distinguish between these patterns. The graphs also have multiple arrows pointing outwards from the center, indicating important information or insights about the hammer time patterns. So this actually gives us a ton of information about images, but it also allows us to go and blend this together. If we wanted to, we could then go and take that text, could go and pass it to another model and blend it all together. So let's say, for example, we want to combine stock prices and candlestick patterns. We could potentially do that. But that in a nutshell, wraps it up. We've now gone and used a multimodal model. And we've also gone through all of the five steps when it comes to going ahead and using Llama's CPP OpenAI server or the fake OpenAI server. Gotcha.
